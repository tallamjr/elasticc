#!/bin/bash -l
# Copyright 2023
# Author: Tarek Allam Jr.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# Batch script to run a serial job under SGE.
# https://www.rc.ucl.ac.uk/docs/Job_Results/#qsub-emailing
#$ -m base

# Request a number of GPU cards, (V100:EF - 2 max, A100:L - 4 max)
#$ -l gpu=1

# Request specific GPU types (V100:EF, A100:L)
#$ -ac allow=EF

# Request ten minutes of wallclock time (format hours:minutes:seconds).
#$ -l h_rt=47:10:0

# Request 1 gigabyte of RAM (must be an integer followed by M, G, or T)
#$ -l mem=60G

# Request 15 gigabyte of TMPDIR space (default is 10 GB - remove if cluster is diskless)
#$ -l tmpfs=15G

## #$ -o logs/$JOB_ID.log

# Combine stdout with stderr
#$ -j yes

# Set the name of the job.
#$ -N t3-train-elasticc

# Set the working directory to somewhere in your scratch space.
#  This is a necessary step as compute nodes cannot write to $HOME.
# Replace "<your_UCL_id>" with your UCL user ID.
#$ -wd /home/zcicg57/Scratch/workspace

# Your work should be done in $TMPDIR
cd $TMPDIR

# https://www.rc.ucl.ac.uk/docs/Clusters/Myriad/#tensorflow
module -f unload compilers mpi gcc-libs
module load beta-modules
module load gcc-libs/10.2.0
module load python/3.9.6-gnu-10.2.0
module load cuda/11.2.0/gnu-10.2.0
module load cudnn/8.1.0.77/cuda-11.2
# module load tensorflow/2.11.0/gpu

# Run the application
SECONDS=0 # https://stackoverflow.com/a/8903280/4521950

export ROOT="/scratch/scratch/zcicg57/elasticc"
export ASNWD="/scratch/scratch/zcicg57/astronet"

export PATH="$HOME/miniconda3/envs/elasticc/bin:$PATH"
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib # https://github.com/tensorflow/tensorflow/issues/52988#issue-1047548284
export CUDA_VISIBLE_DEVICES=0,1 # https://stackoverflow.com/a/48079860/4521950
# export TF_CPP_MIN_VLOG_LEVEL=3 # https://stackoverflow.com/q/66118532/4521950, https://stackoverflow.com/a/45142985/4521950
export TF_CPP_MIN_LOG_LEVEL=2
export TF_XLA_FLAGS=--tf_xla_enable_xla_devices # https://github.com/tensorflow/tensorflow/issues/46479
conda activate elasticc
conda env config vars set LD_LIBRARY_PATH=$CONDA_PREFIX/lib # https://github.com/tensorflow/tensorflow/issues/52988#issuecomment-1024604306
conda activate base # To make your changes take effect reactivating elasticc environment
conda activate elasticc
which python
which pip

CUDNN_PATH=$(dirname $(python -c "import nvidia.cudnn;print(nvidia.cudnn.__file__)"))
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$CONDA_PREFIX/lib/:$CUDNN_PATH/lib
echo $LD_LIBRARY_PATH
# Verify install:
python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
# Log GPU information
nvidia-smi

export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX
echo $XLA_FLAGS

pip install --no-cache-dir --ignore-installed -r $ASNWD/requirements.txt
pip install --no-cache-dir --ignore-installed -r $ROOT/requirements.txt
pip install $ROOT

export PYTHONHASHSEED=0
which python
date
# Test Imports
python -c "import astronet as asn; print(asn.__version__)"
python -c "import tensorflow as tf; print(tf.__version__);"
python -c "import tensorflow as tf; print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))"

operation=$1 # i.e. {train, hyper}
# Architecture
## Current options: {tinho, t2, atx}
architecture=$2
echo "Running $operation using $architecture architecture"

dataset="elasticc"

if [ "$operation" == "train" ]; then
    # Train
    case "$architecture" in
     "atx") model="scaledown-by-4" ;;
     *) model="1613517996-0a72904" ;; # t2 and tinho use same starting hyperparameters
     # *) model="fix" ;; # trial different set of params
    esac
    python $ROOT/elasticc/train.py \
        --architecture $architecture \
        --dataset $dataset \
        --epochs 200 \
        --model $model \
        # --redshift ""
elif
  # HyperTrain
  case "$architecture" in
   "atx") model="scaledown-by-4" ;;
   *) model="1613517996-0a72904" ;; # t2 and tinho use same starting hyperparameters
   # *) model="fix" ;; # trial different set of params
  esac
  python $ROOT/elasticc/hypertrain.py \
      --dataset $dataset \
      --epochs 40 \
      --num-trials 5 \
else
    echo "Please provide an argument for $operation"
fi
date
duration=$SECONDS
echo "$(($duration / 60)) minutes and $(($duration % 60)) seconds elapsed."
